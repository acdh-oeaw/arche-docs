While using metadata read modes other than `resource` you can easily run into downloading a massive amounts of metadata.
You should be aware that the metadata serialization format heavily affects the API performance.

If you just want to know the best solutions:

* If it's another software talking to the API, use `application/n-triples`. It's definitely the fastest to serialize on the ARCHE side and most probably the fastest to parse on your application side (with the only exception of JS where `application/ld+json` is probably the fastest to parse).
* If it's a human being looking at the API output, use `text/turtle`.

# Test results

A few tests were performed on ACDH's ARCHE production instance.
The repository contained 85k resources and 2.6M triples.

Results for `application/rdf+xml` and `application/ld+json` are the same for ARCHE 1.9 and 1.10.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=8}
# time curl 'https://arche.acdh.oeaw.ac.at/api/46266/metadata?readMode=neighbors&parentProperty=https%3A%2F%2Fvocabs.acdh.oeaw.ac.at%2Fschema%23isPartOf&format=text/turtle' > /dev/null
library(magrittr)
d = readr::read_csv2('metadata_api_performance.csv') %>%
  dplyr::mutate(
    "ARCHE version" = dplyr::if_else(serializer == 'easyrdf', '1.9', '1.10'),
    scenario = paste(format, serializer)
  )
d %>% 
  dplyr::filter(time < 250) %>%
  ggplot2::ggplot(ggplot2::aes(x = `triples count`, y = time, color = format, linetype = `ARCHE version`, group = scenario)) +
  ggplot2::geom_line() +
  ggplot2::ggtitle('Response time vs triples count') +
  ggplot2::theme_light()
d %>% 
  ggplot2::ggplot(ggplot2::aes(x = `triples count`, y = `memory usage`, color = format, linetype = `ARCHE version`, group = scenario)) +
  ggplot2::geom_line() +
  ggplot2::ggtitle('Memory usage vs triples count') +
  ggplot2::theme_light()
```

As we can see:

* The most performant format is `application/n-triples`. Since ARCHE 1.10.0 it has a constant memory footprint, therefore there is no risk of out of memory error on the server side. On the test machine it was able to serialize up to around 0.5M triples within PHP's standard 30s script execution time limit.
* The second most performant format is `text/turtle`. It takes roughly twice as much time to generate a `text/turtle` response than `application/n-triples` one (as of ARCHE 1.10) but it's worth noticing it has much higher server side memory footprint (here up to around 100MB @ 150k triples).
* `application/ld+json` is significantly slower (more than twice comparing to `text/turtle` and more than four times comparing to `application/n-triples`) and has much bigger server-side memory footprint (almost 350MB @ 150k triples comparing to around 100MB for `text/turtle` and 2MB of `application/n-triples`). As there are `application/n-triples` parsers for JS, in large triples count scenarios it's more performant and safe (no risk of out of memory and minimal risk of execution timeout) to use `application/n-triples` as the data interchange format and the parser on the client side.
* `rdf+xml` is definitely the worse format as the serialization time grows exponentially with the number of triples. Please just avoid it.

# Theory

* `application/n-triples` should be the fastest and has neglectable server memory footprint as every triple read from the database can be just immidiately streamed to the output. Also the amount of processing is minimal and goes down to some basic escaping only.
* A nicely formatted `text/turtle` is more demanding. First, whole data must be inspected first to create prefixes list. Second, data must be either buffered or initially ordered by the subject, so triples can be grouped by subject (and property). It's still pretty fast but as data has to be buffered, `text/turtle` memory footprint grows linearly with the number of return triples.
* `application/ld+json` suffers from same problems as `text/turtle` (we also expect linear relation between number of triples and both serialization time and server memory footprint), just the library stack used to generate it in ARCHE is less performant (EasyRdf and ml/json-ld used for `application/ld+json` introduces bigger overhead than ARCHE-embedded preprocessing + pietercolpaert/hardf used for `text/turtle`).
* `application/rdf+xml` uses EasyRdf XML serializer which is just slow.
